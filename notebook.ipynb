{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41bd907f",
   "metadata": {},
   "source": [
    "# ADAML workshop 3: Recurrent Neural Network\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f421282",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## Model implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7994c8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class RNN:\n",
    "    def __init__(self, input_size, hidden_size, output_size, lr=1e-3, seed=1):\n",
    "        rng = np.random.RandomState(seed)\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.Wxh = rng.randn(hidden_size, input_size)\n",
    "        self.Whh = rng.randn(hidden_size, hidden_size)\n",
    "        self.Why = rng.randn(output_size, hidden_size)\n",
    "        self.bh = np.zeros((hidden_size, 1))\n",
    "        self.by = np.zeros((output_size, 1))\n",
    "        self.lr = lr\n",
    "\n",
    "    def forward(self, X):\n",
    "        # In forward model we feed the data X through the network.\n",
    "        # X shape: (batch, seq_len, input_size)\n",
    "\n",
    "        batch, seq_len, _ = X.shape\n",
    "        h = np.zeros((batch, seq_len + 1, self.hidden_size))\n",
    "        for t in range(seq_len):\n",
    "            # Reshaping to correct input shape (batch, input_size)\n",
    "            xt = X[:, t, :].reshape(batch, -1)\n",
    "            # compute next hidden: h_t = tanh(Wxh@x_t + Whh@h_{t-1} + bh)\n",
    "            pre = xt.dot(self.Wxh.T) + h[:, t, :].dot(self.Whh.T) + self.bh.T\n",
    "            h[:, t + 1, :] = np.tanh(pre)\n",
    "        # compute output using the last hidden state.\n",
    "        # NOTE:  We could also use multiple hidden states to have more context.\n",
    "        y_pred = h[:, -1, :].dot(self.Why.T) + self.by.T\n",
    "        return h, y_pred  # h includes initial zero state at index 0\n",
    "\n",
    "    def loss(self, y_pred, y_true):\n",
    "        # MSE\n",
    "        diff = y_pred - y_true\n",
    "        return np.mean(diff**2), diff\n",
    "\n",
    "    def bptt_update(self, X, h, y_pred, y_true):\n",
    "        # Using backpropagation through time (bptt) to update the weights\n",
    "        # X: (batch, seq_len, input_size)\n",
    "        batch, seq_len, _ = X.shape\n",
    "\n",
    "        # Initializing the gradients\n",
    "        dWxh = np.zeros_like(self.Wxh)\n",
    "        dWhh = np.zeros_like(self.Whh)\n",
    "        dWhy = np.zeros_like(self.Why)\n",
    "        dbh = np.zeros_like(self.bh)\n",
    "        dby = np.zeros_like(self.by)\n",
    "\n",
    "        # dy on outputs (MSE derivative)\n",
    "        dy = (y_pred - y_true) * (2.0 / batch)  # shape (batch, output_size)\n",
    "        # dWhy and dby from last hidden\n",
    "        # (batch, hidden)\n",
    "        h_last = h[:, -1, :].reshape(batch, self.hidden_size)\n",
    "        dWhy += dy.T.dot(h_last)  # (output, hidden)\n",
    "        dby += dy.T.sum(axis=1, keepdims=True)  # (output,1)\n",
    "\n",
    "        # backprop into last hidden state\n",
    "        dh_next = dy.dot(self.Why)  # (batch, hidden)\n",
    "\n",
    "        # BPTT through time\n",
    "        # NOTE: As in normal BP, we go the network backwards\n",
    "        for t in reversed(range(seq_len)):\n",
    "            ht = h[:, t + 1, :]  # (batch, hidden)\n",
    "            ht_prev = h[:, t, :]  # (batch, hidden)\n",
    "            # derivative through tanh\n",
    "            dt = dh_next * (1 - ht**2)  # (batch, hidden)\n",
    "            dbh += dt.T.sum(axis=1, keepdims=True)\n",
    "            # dWxh: sum over batch of dt^T x_t\n",
    "            xt = X[:, t, :].reshape(batch, -1)\n",
    "            dWxh += dt.T.dot(xt)  # (hidden, input)\n",
    "            # dWhh: dt^T h_{t-1}\n",
    "            dWhh += dt.T.dot(ht_prev)\n",
    "            # propagate dh to previous time step\n",
    "            dh_next = dt.dot(self.Whh)\n",
    "\n",
    "        # Gradient clipping to avoid exploding gradients.\n",
    "        for grad in (dWxh, dWhh, dWhy, dbh, dby):\n",
    "            np.clip(grad, -5, 5, out=grad)\n",
    "\n",
    "        # SGD parameter update\n",
    "        self.Wxh -= self.lr * dWxh\n",
    "        self.Whh -= self.lr * dWhh\n",
    "        self.Why -= self.lr * dWhy\n",
    "        self.bh -= self.lr * dbh\n",
    "        self.by -= self.lr * dby\n",
    "\n",
    "    def train(self, X, y, epochs=50, batch_size=32, verbose=True):\n",
    "        n = X.shape[0]\n",
    "        losses = []\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            # shuffle\n",
    "            idx = np.random.permutation(n)\n",
    "            X_shuffled = X[idx]\n",
    "            y_shuffled = y[idx]\n",
    "            epoch_loss = 0.0\n",
    "\n",
    "            # Creating batches, feeding through the network,\n",
    "            # computing loss, and updating the gradients\n",
    "            for i in range(0, n, batch_size):\n",
    "                xb = X_shuffled[i: i + batch_size]\n",
    "                yb = y_shuffled[i: i + batch_size]\n",
    "                h, y_pred = self.forward(xb)\n",
    "                loss, _ = self.loss(y_pred, yb)\n",
    "                epoch_loss += loss * xb.shape[0]\n",
    "                self.bptt_update(xb, h, y_pred, yb)\n",
    "            epoch_loss /= n\n",
    "            losses.append(epoch_loss)\n",
    "            if verbose and (epoch % max(1, epochs // 10) == 0 or epoch == 1):\n",
    "                print(f\"Epoch {epoch}/{epochs} - loss: {epoch_loss:.6f}\")\n",
    "        return losses\n",
    "\n",
    "def generate_sine_sequences(n_samples=2000, seq_len=20, input_size=1, seed=0):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    x = np.linspace(0, 50, n_samples * seq_len * input_size)\n",
    "    data = np.sin(x) + 0.1 * rng.randn(n_samples * seq_len * input_size)\n",
    "    X = data.reshape(n_samples, seq_len, input_size)\n",
    "    rolled = np.roll(data, -1).reshape(n_samples, seq_len, input_size)\n",
    "    y_last = rolled[:, -1, :]\n",
    "    return X.astype(np.float32), y_last.astype(np.float32)\n",
    "\n",
    "if False: \n",
    "    # Creating the data\n",
    "    X, y = generate_sine_sequences(n_samples=1500, seq_len=20, input_size=1)\n",
    "    # train/test split\n",
    "    split = int(0.8 * X.shape[0])\n",
    "    X_train, y_train = X[:split], y[:split]\n",
    "    X_test, y_test = X[split:], y[split:]\n",
    "\n",
    "    print(X_train.shape, y_train.shape)\n",
    "\n",
    "    # Training the network\n",
    "    rnn = RNN(input_size=1, hidden_size=80, output_size=1, lr=1e-3)\n",
    "    losses = rnn.train(X_train, y_train, epochs=600, batch_size=8, verbose=True)\n",
    "\n",
    "    # Test set\n",
    "    _, y_pred_test = rnn.forward(X_test)\n",
    "    test_loss, _ = rnn.loss(y_pred_test, y_test)\n",
    "    print(f\"\\nTest MSE: {test_loss:.6f}\\n\")\n",
    "\n",
    "    # Plotting the predictions.\n",
    "    plt.plot(y_test)\n",
    "    plt.plot(y_pred_test)\n",
    "    plt.savefig(\"rnn_pred.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45e2808",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## Classification model implementation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "48876ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ClassificationRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, output_size):\n",
    "        super(ClassificationRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.RNN(embed_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.hidden_size, self.output_size = hidden_size, output_size\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92df6eab",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## Data onboarding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caff083a",
   "metadata": {},
   "source": [
    "Fetch the data from kaggle API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c14397a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2225, 2)\n",
      "                                                Text  Label\n",
      "0  Budget to set scene for election\\n \\n Gordon B...      0\n",
      "1  Army chiefs in regiments decision\\n \\n Militar...      0\n",
      "2  Howard denies split over ID cards\\n \\n Michael...      0\n",
      "3  Observers to monitor UK election\\n \\n Minister...      0\n",
      "4  Kilroy names election seat target\\n \\n Ex-chat...      0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import kagglehub\n",
    "\n",
    "# Fetch the latest version of the dataset from kaggle\n",
    "data_dir = kagglehub.dataset_download(\"tanishqdublish/text-classification-documentation\")\n",
    "data_path = os.path.join(data_dir, os.listdir(data_dir)[0]);\n",
    "\n",
    "data = pd.read_csv(data_path)\n",
    "print(data.shape)\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae6f926",
   "metadata": {},
   "source": [
    "Split the data to features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cc3305cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data['Text'], data['Label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15fc008",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7170ce72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sentences to lowercase\n",
    "X = X.str.lower().str.split()\n",
    "\n",
    "# \n",
    "vocab = {word for phrase in X for word in phrase}\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocab, start=1)}\n",
    "\n",
    "max_length = X.str.len().max()\n",
    "\n",
    "def encode_and_pad(text):\n",
    "    encoded = [word_to_idx[word] for word in text]\n",
    "    return encoded + [0] * (max_length - len(encoded))\n",
    "\n",
    "X = X.apply(encode_and_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b0a05d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/eliaseskelinen/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text \u001b[38;5;66;03m#' '.join(words)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Apply the cleaning function\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mapply(clean_text)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/pandas/core/series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4800\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\n\u001b[1;32m   4918\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4919\u001b[0m         func,\n\u001b[1;32m   4920\u001b[0m         convert_dtype\u001b[38;5;241m=\u001b[39mconvert_dtype,\n\u001b[1;32m   4921\u001b[0m         by_row\u001b[38;5;241m=\u001b[39mby_row,\n\u001b[1;32m   4922\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   4923\u001b[0m         kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m-> 4924\u001b[0m     )\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/pandas/core/apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/pandas/core/apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_map_values(\n\u001b[1;32m   1508\u001b[0m     mapper\u001b[38;5;241m=\u001b[39mcurried, na_action\u001b[38;5;241m=\u001b[39maction, convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype\n\u001b[1;32m   1509\u001b[0m )\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m algorithms\u001b[38;5;241m.\u001b[39mmap_array(arr, mapper, na_action\u001b[38;5;241m=\u001b[39mna_action, convert\u001b[38;5;241m=\u001b[39mconvert)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer(values, mapper, convert\u001b[38;5;241m=\u001b[39mconvert)\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[41], line 12\u001b[0m, in \u001b[0;36mclean_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mclean_text\u001b[39m(text):\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# Lowercase and remove non-alphabetic characters\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m     13\u001b[0m     text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[^a-z\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n\u001b[1;32m     14\u001b[0m     text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms+\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, text)\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "import re \n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "\n",
    "    # Lowercase and remove non-alphabetic characters\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    words = text.split()\n",
    "\n",
    "    # Lemmatization\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "    # Stop-word removal\n",
    "    # words = [word for word in words if word not in stopwords]\n",
    "\n",
    "    return text #' '.join(words)\n",
    "\n",
    "# Apply the cleaning function\n",
    "X = X.apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de2d5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# import re\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# # # Tokenize each sentence\n",
    "# # for i, x in enumerate(X): \n",
    "# #     #tokens = nltk.word_tokenize(x)\n",
    "# #     tokens = nltk.word_tokenize(x)\n",
    "# #     X[i] = ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d7b378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2225,)\n"
     ]
    }
   ],
   "source": [
    "X, y = X.to_numpy(), y.to_numpy()\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b63f57",
   "metadata": {},
   "source": [
    "Vectorize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c92b505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Basic tokenization\n",
    "def tokenize(sentence):\n",
    "    return re.findall(r\"\\b\\w+\\b\", sentence.lower())\n",
    "\n",
    "tokenized = [tokenize(s) for s in X]\n",
    "\n",
    "# Build vocabulary\n",
    "vocab = {}\n",
    "for sent in tokenized:\n",
    "    for word in sent:\n",
    "        if word not in vocab:\n",
    "            vocab[word] = len(vocab) + 1  # start indexing at 1\n",
    "\n",
    "sequences = [[vocab[word] for word in sent] for sent in tokenized]\n",
    "# e.g., [[1, 2, 3, 4, 1, 5], [6, 7, 8, 9], [10, 11, 12]]\n",
    "\n",
    "max_len = max(len(seq) for seq in sequences)\n",
    "padded = np.zeros((len(sequences), max_len), dtype=int)\n",
    "\n",
    "for i, seq in enumerate(sequences):\n",
    "    padded[i, :len(seq)] = seq\n",
    "# shape = (batch, seq_len)\n",
    "\n",
    "vocab_size = len(vocab) + 1  # +1 for padding index 0\n",
    "embedding_dim = 300  # features per token\n",
    "\n",
    "embedding_matrix = np.random.randn(vocab_size, embedding_dim)\n",
    "embeddings = embedding_matrix[padded]  # shape = (batch, seq_len, features)\n",
    "\n",
    "rnn_input = np.transpose(embeddings, (2, 1, 0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047870d0",
   "metadata": {},
   "source": [
    "Split the data to training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1731b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(rnn_input, y, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a64ad6f",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac8e786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 4396, 2225) (2225,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (32,2225) and (1,80) not aligned: 2225 (dim 1) != 1 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m rnn \u001b[38;5;241m=\u001b[39m RNN(input_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, hidden_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m80\u001b[39m, output_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(rnn_input\u001b[38;5;241m.\u001b[39mshape, y\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m----> 4\u001b[0m rnn\u001b[38;5;241m.\u001b[39mtrain(rnn_input, y)\n",
      "Cell \u001b[0;32mIn[26], line 104\u001b[0m, in \u001b[0;36mRNN.train\u001b[0;34m(self, X, y, epochs, batch_size, verbose)\u001b[0m\n\u001b[1;32m    102\u001b[0m xb \u001b[38;5;241m=\u001b[39m X_shuffled[i: i \u001b[38;5;241m+\u001b[39m batch_size]\n\u001b[1;32m    103\u001b[0m yb \u001b[38;5;241m=\u001b[39m y_shuffled[i: i \u001b[38;5;241m+\u001b[39m batch_size]\n\u001b[0;32m--> 104\u001b[0m h, y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(xb)\n\u001b[1;32m    105\u001b[0m loss, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(y_pred, yb)\n\u001b[1;32m    106\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m*\u001b[39m xb\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[0;32mIn[26], line 27\u001b[0m, in \u001b[0;36mRNN.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     25\u001b[0m     xt \u001b[38;5;241m=\u001b[39m X[:, t, :]\u001b[38;5;241m.\u001b[39mreshape(batch, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# compute next hidden: h_t = tanh(Wxh@x_t + Whh@h_{t-1} + bh)\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m     pre \u001b[38;5;241m=\u001b[39m xt\u001b[38;5;241m.\u001b[39mdot(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mWxh\u001b[38;5;241m.\u001b[39mT) \u001b[38;5;241m+\u001b[39m h[:, t, :]\u001b[38;5;241m.\u001b[39mdot(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mWhh\u001b[38;5;241m.\u001b[39mT) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbh\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m     28\u001b[0m     h[:, t \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, :] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtanh(pre)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# compute output using the last hidden state.\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# NOTE:  We could also use multiple hidden states to have more context.\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (32,2225) and (1,80) not aligned: 2225 (dim 1) != 1 (dim 0)"
     ]
    }
   ],
   "source": [
    "rnn = RNN(input_size=1, hidden_size=80, output_size=1, lr=1e-3)\n",
    "\n",
    "print(rnn_input.shape, y.shape)\n",
    "rnn.train(rnn_input, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e73985",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/content/IMDB Dataset.csv\", names=[\"text\", \"label\"])\n",
    "\n",
    "le = LabelEncoder()\n",
    "df['label'] = le.fit_transform(df['label'])\n",
    "\n",
    "train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "vocab = {word for phrase in df['text'] for word in phrase}\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocab, start=1)}\n",
    "\n",
    "max_length = df['text'].str.len().max()\n",
    "\n",
    "def encode_and_pad(text):\n",
    "    encoded = [word_to_idx[word] for word in text]\n",
    "    return encoded + [0] * (max_length - len(encoded))\n",
    "\n",
    "train_data['text'] = train_data['text'].apply(encode_and_pad)\n",
    "test_data['text'] = test_data['text'].apply(encode_and_pad)\n",
    "vocab_size = len(vocab) + 1\n",
    "embed_size = 128\n",
    "hidden_size = 128\n",
    "output_size = 2 \n",
    "model = ClassificationRNN(vocab_size, embed_size, hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22aa8c3",
   "metadata": {},
   "source": [
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
