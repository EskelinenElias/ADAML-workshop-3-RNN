{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41bd907f",
   "metadata": {},
   "source": [
    "# ADAML workshop 3: Recurrent Neural Network\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f421282",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## Model implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7994c8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class RNN:\n",
    "    def __init__(self, input_size, hidden_size, output_size, lr=1e-3, seed=1):\n",
    "        rng = np.random.RandomState(seed)\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.Wxh = rng.randn(hidden_size, input_size)\n",
    "        self.Whh = rng.randn(hidden_size, hidden_size)\n",
    "        self.Why = rng.randn(output_size, hidden_size)\n",
    "        self.bh = np.zeros((hidden_size, 1))\n",
    "        self.by = np.zeros((output_size, 1))\n",
    "        self.lr = lr\n",
    "\n",
    "    def forward(self, X):\n",
    "        # In forward model we feed the data X through the network.\n",
    "        # X shape: (batch, seq_len, input_size)\n",
    "\n",
    "        batch, seq_len, _ = X.shape\n",
    "        h = np.zeros((batch, seq_len + 1, self.hidden_size))\n",
    "        for t in range(seq_len):\n",
    "            # Reshaping to correct input shape (batch, input_size)\n",
    "            xt = X[:, t, :].reshape(batch, -1)\n",
    "            # compute next hidden: h_t = tanh(Wxh@x_t + Whh@h_{t-1} + bh)\n",
    "            pre = xt.dot(self.Wxh.T) + h[:, t, :].dot(self.Whh.T) + self.bh.T\n",
    "            h[:, t + 1, :] = np.tanh(pre)\n",
    "        # compute output using the last hidden state.\n",
    "        # NOTE:  We could also use multiple hidden states to have more context.\n",
    "        y_pred = h[:, -1, :].dot(self.Why.T) + self.by.T\n",
    "        return h, y_pred  # h includes initial zero state at index 0\n",
    "\n",
    "    def loss(self, y_pred, y_true):\n",
    "        # MSE\n",
    "        diff = y_pred - y_true\n",
    "        return np.mean(diff**2), diff\n",
    "\n",
    "    def bptt_update(self, X, h, y_pred, y_true):\n",
    "        # Using backpropagation through time (bptt) to update the weights\n",
    "        # X: (batch, seq_len, input_size)\n",
    "        batch, seq_len, _ = X.shape\n",
    "\n",
    "        # Initializing the gradients\n",
    "        dWxh = np.zeros_like(self.Wxh)\n",
    "        dWhh = np.zeros_like(self.Whh)\n",
    "        dWhy = np.zeros_like(self.Why)\n",
    "        dbh = np.zeros_like(self.bh)\n",
    "        dby = np.zeros_like(self.by)\n",
    "\n",
    "        # dy on outputs (MSE derivative)\n",
    "        dy = (y_pred - y_true) * (2.0 / batch)  # shape (batch, output_size)\n",
    "        # dWhy and dby from last hidden\n",
    "        # (batch, hidden)\n",
    "        h_last = h[:, -1, :].reshape(batch, self.hidden_size)\n",
    "        dWhy += dy.T.dot(h_last)  # (output, hidden)\n",
    "        dby += dy.T.sum(axis=1, keepdims=True)  # (output,1)\n",
    "\n",
    "        # backprop into last hidden state\n",
    "        dh_next = dy.dot(self.Why)  # (batch, hidden)\n",
    "\n",
    "        # BPTT through time\n",
    "        # NOTE: As in normal BP, we go the network backwards\n",
    "        for t in reversed(range(seq_len)):\n",
    "            ht = h[:, t + 1, :]  # (batch, hidden)\n",
    "            ht_prev = h[:, t, :]  # (batch, hidden)\n",
    "            # derivative through tanh\n",
    "            dt = dh_next * (1 - ht**2)  # (batch, hidden)\n",
    "            dbh += dt.T.sum(axis=1, keepdims=True)\n",
    "            # dWxh: sum over batch of dt^T x_t\n",
    "            xt = X[:, t, :].reshape(batch, -1)\n",
    "            dWxh += dt.T.dot(xt)  # (hidden, input)\n",
    "            # dWhh: dt^T h_{t-1}\n",
    "            dWhh += dt.T.dot(ht_prev)\n",
    "            # propagate dh to previous time step\n",
    "            dh_next = dt.dot(self.Whh)\n",
    "\n",
    "        # Gradient clipping to avoid exploding gradients.\n",
    "        for grad in (dWxh, dWhh, dWhy, dbh, dby):\n",
    "            np.clip(grad, -5, 5, out=grad)\n",
    "\n",
    "        # SGD parameter update\n",
    "        self.Wxh -= self.lr * dWxh\n",
    "        self.Whh -= self.lr * dWhh\n",
    "        self.Why -= self.lr * dWhy\n",
    "        self.bh -= self.lr * dbh\n",
    "        self.by -= self.lr * dby\n",
    "\n",
    "    def train(self, X, y, epochs=50, batch_size=32, verbose=True):\n",
    "        n = X.shape[0]\n",
    "        losses = []\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            # shuffle\n",
    "            idx = np.random.permutation(n)\n",
    "            X_shuffled = X[idx]\n",
    "            y_shuffled = y[idx]\n",
    "            epoch_loss = 0.0\n",
    "\n",
    "            # Creating batches, feeding through the network,\n",
    "            # computing loss, and updating the gradients\n",
    "            for i in range(0, n, batch_size):\n",
    "                xb = X_shuffled[i: i + batch_size]\n",
    "                yb = y_shuffled[i: i + batch_size]\n",
    "                h, y_pred = self.forward(xb)\n",
    "                loss, _ = self.loss(y_pred, yb)\n",
    "                epoch_loss += loss * xb.shape[0]\n",
    "                self.bptt_update(xb, h, y_pred, yb)\n",
    "            epoch_loss /= n\n",
    "            losses.append(epoch_loss)\n",
    "            if verbose and (epoch % max(1, epochs // 10) == 0 or epoch == 1):\n",
    "                print(f\"Epoch {epoch}/{epochs} - loss: {epoch_loss:.6f}\")\n",
    "        return losses\n",
    "\n",
    "def generate_sine_sequences(n_samples=2000, seq_len=20, input_size=1, seed=0):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    x = np.linspace(0, 50, n_samples * seq_len * input_size)\n",
    "    data = np.sin(x) + 0.1 * rng.randn(n_samples * seq_len * input_size)\n",
    "    X = data.reshape(n_samples, seq_len, input_size)\n",
    "    rolled = np.roll(data, -1).reshape(n_samples, seq_len, input_size)\n",
    "    y_last = rolled[:, -1, :]\n",
    "    return X.astype(np.float32), y_last.astype(np.float32)\n",
    "\n",
    "if False: \n",
    "    # Creating the data\n",
    "    X, y = generate_sine_sequences(n_samples=1500, seq_len=20, input_size=1)\n",
    "    # train/test split\n",
    "    split = int(0.8 * X.shape[0])\n",
    "    X_train, y_train = X[:split], y[:split]\n",
    "    X_test, y_test = X[split:], y[split:]\n",
    "\n",
    "    print(X_train.shape, y_train.shape)\n",
    "\n",
    "    # Training the network\n",
    "    rnn = RNN(input_size=1, hidden_size=80, output_size=1, lr=1e-3)\n",
    "    losses = rnn.train(X_train, y_train, epochs=600, batch_size=8, verbose=True)\n",
    "\n",
    "    # Test set\n",
    "    _, y_pred_test = rnn.forward(X_test)\n",
    "    test_loss, _ = rnn.loss(y_pred_test, y_test)\n",
    "    print(f\"\\nTest MSE: {test_loss:.6f}\\n\")\n",
    "\n",
    "    # Plotting the predictions.\n",
    "    plt.plot(y_test)\n",
    "    plt.plot(y_pred_test)\n",
    "    plt.savefig(\"rnn_pred.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45e2808",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## Classification model implementation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48876ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ClassificationRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, output_size):\n",
    "        super(ClassificationRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.RNN(embed_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.hidden_size, self.output_size = hidden_size, output_size\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92df6eab",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## Data onboarding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caff083a",
   "metadata": {},
   "source": [
    "Fetch the data from kaggle API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c14397a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2225, 2)\n",
      "                                                Text  Label\n",
      "0  Budget to set scene for election\\n \\n Gordon B...      0\n",
      "1  Army chiefs in regiments decision\\n \\n Militar...      0\n",
      "2  Howard denies split over ID cards\\n \\n Michael...      0\n",
      "3  Observers to monitor UK election\\n \\n Minister...      0\n",
      "4  Kilroy names election seat target\\n \\n Ex-chat...      0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import kagglehub\n",
    "\n",
    "# Fetch the latest version of the dataset from kaggle\n",
    "data_dir = kagglehub.dataset_download(\"tanishqdublish/text-classification-documentation\")\n",
    "data_path = os.path.join(data_dir, os.listdir(data_dir)[0]);\n",
    "\n",
    "data = pd.read_csv(data_path)\n",
    "print(data.shape)\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae6f926",
   "metadata": {},
   "source": [
    "Split the data to features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc3305cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data['Text'], data['Label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15fc008",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281da23a",
   "metadata": {},
   "source": [
    "Clean the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e1e489",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Keep only letters and basic punctuation\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \" \", text)\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "X_clean = X.apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096092ef",
   "metadata": {},
   "source": [
    "Tokenize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7170ce72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokens = re.findall(r\"\\b\\w+\\b\", text.lower())\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "X_tokenized = X_clean.apply(tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c9e072",
   "metadata": {},
   "source": [
    "Vectorize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "21b6c037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27880\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Join tokens back into sentences for sklearn\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(X_tokenized)\n",
    "\n",
    "# Vocabulary: word â†’ integer index\n",
    "vocab = vectorizer.vocabulary_\n",
    "print(len(vocab))\n",
    "\n",
    "# Encode sentences\n",
    "X_encoded = [vectorizer.transform([s]).toarray()[0] for s in X_tokenized]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdef8b3",
   "metadata": {},
   "source": [
    "Padding the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e7b18e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27880\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "max_len = max(len(seq) for seq in X_encoded)\n",
    "print(max_len)\n",
    "\n",
    "def pad_sequences_numpy(sequences, max_len, pad_value=0):\n",
    "    batch_size = len(sequences)\n",
    "    padded_seqs = np.full((batch_size, max_len), pad_value, dtype=int)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        padded_seqs[i, :len(seq)] = seq\n",
    "    return padded_seqs\n",
    "\n",
    "X_padded = pad_sequences_numpy(X_encoded, max_len)\n",
    "print(X_padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0154011",
   "metadata": {},
   "source": [
    "Embedding the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8e38968d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_embedded shape: (2225, 27880, 8)\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 8\n",
    "\n",
    "# Random embeddings: shape (vocab_size, embedding_dim)\n",
    "embedding_matrix = np.random.randn(len(vocab), embedding_dim)\n",
    "\n",
    "# \n",
    "batch_size, seq_len = X_padded.shape\n",
    "\n",
    "# Create an empty array for embeddings\n",
    "X_embedded = np.zeros((batch_size, seq_len, embedding_dim))\n",
    "\n",
    "for i in range(batch_size):\n",
    "    for j in range(seq_len):\n",
    "        token_id = X_padded[i, j]\n",
    "        X_embedded[i, j] = embedding_matrix[token_id]\n",
    "\n",
    "print(\"X_embedded shape:\", X_embedded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047870d0",
   "metadata": {},
   "source": [
    "Split the data to training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c1731b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1780, 27880, 8)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_embedded, y, test_size=0.2)\n",
    "\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a64ad6f",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dac8e786",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (32,8) and (1,80) not aligned: 8 (dim 1) != 1 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m rnn \u001b[38;5;241m=\u001b[39m RNN(input_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, hidden_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m80\u001b[39m, output_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m rnn\u001b[38;5;241m.\u001b[39mtrain(X_train, y_train)\n",
      "Cell \u001b[0;32mIn[15], line 104\u001b[0m, in \u001b[0;36mRNN.train\u001b[0;34m(self, X, y, epochs, batch_size, verbose)\u001b[0m\n\u001b[1;32m    102\u001b[0m xb \u001b[38;5;241m=\u001b[39m X_shuffled[i: i \u001b[38;5;241m+\u001b[39m batch_size]\n\u001b[1;32m    103\u001b[0m yb \u001b[38;5;241m=\u001b[39m y_shuffled[i: i \u001b[38;5;241m+\u001b[39m batch_size]\n\u001b[0;32m--> 104\u001b[0m h, y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(xb)\n\u001b[1;32m    105\u001b[0m loss, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(y_pred, yb)\n\u001b[1;32m    106\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m*\u001b[39m xb\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[0;32mIn[15], line 27\u001b[0m, in \u001b[0;36mRNN.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     25\u001b[0m     xt \u001b[38;5;241m=\u001b[39m X[:, t, :]\u001b[38;5;241m.\u001b[39mreshape(batch, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# compute next hidden: h_t = tanh(Wxh@x_t + Whh@h_{t-1} + bh)\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m     pre \u001b[38;5;241m=\u001b[39m xt\u001b[38;5;241m.\u001b[39mdot(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mWxh\u001b[38;5;241m.\u001b[39mT) \u001b[38;5;241m+\u001b[39m h[:, t, :]\u001b[38;5;241m.\u001b[39mdot(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mWhh\u001b[38;5;241m.\u001b[39mT) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbh\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m     28\u001b[0m     h[:, t \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, :] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtanh(pre)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# compute output using the last hidden state.\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# NOTE:  We could also use multiple hidden states to have more context.\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (32,8) and (1,80) not aligned: 8 (dim 1) != 1 (dim 0)"
     ]
    }
   ],
   "source": [
    "rnn = RNN(input_size=1, hidden_size=80, output_size=1, lr=1e-3)\n",
    "\n",
    "rnn.train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e73985",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/content/IMDB Dataset.csv\", names=[\"text\", \"label\"])\n",
    "\n",
    "le = LabelEncoder()\n",
    "df['label'] = le.fit_transform(df['label'])\n",
    "\n",
    "train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "vocab = {word for phrase in df['text'] for word in phrase}\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocab, start=1)}\n",
    "\n",
    "max_length = df['text'].str.len().max()\n",
    "\n",
    "def encode_and_pad(text):\n",
    "    encoded = [word_to_idx[word] for word in text]\n",
    "    return encoded + [0] * (max_length - len(encoded))\n",
    "\n",
    "train_data['text'] = train_data['text'].apply(encode_and_pad)\n",
    "test_data['text'] = test_data['text'].apply(encode_and_pad)\n",
    "vocab_size = len(vocab) + 1\n",
    "embed_size = 128\n",
    "hidden_size = 128\n",
    "output_size = 2 \n",
    "model = ClassificationRNN(vocab_size, embed_size, hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22aa8c3",
   "metadata": {},
   "source": [
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
