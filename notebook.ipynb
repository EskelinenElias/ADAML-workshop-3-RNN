{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41bd907f",
   "metadata": {},
   "source": [
    "# ADAML workshop 3: Recurrent Neural Network\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f421282",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## Model implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7994c8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class RNN:\n",
    "    def __init__(self, input_size, hidden_size, output_size, lr=1e-3, seed=1):\n",
    "        rng = np.random.RandomState(seed)\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.Wxh = rng.randn(hidden_size, input_size)\n",
    "        self.Whh = rng.randn(hidden_size, hidden_size)\n",
    "        self.Why = rng.randn(output_size, hidden_size)\n",
    "        self.bh = np.zeros((hidden_size, 1))\n",
    "        self.by = np.zeros((output_size, 1))\n",
    "        self.lr = lr\n",
    "\n",
    "    def forward(self, X):\n",
    "        # In forward model we feed the data X through the network.\n",
    "        # X shape: (batch, seq_len, input_size)\n",
    "\n",
    "        batch, seq_len, _ = X.shape\n",
    "        h = np.zeros((batch, seq_len + 1, self.hidden_size))\n",
    "        for t in range(seq_len):\n",
    "            # Reshaping to correct input shape (batch, input_size)\n",
    "            xt = X[:, t, :].reshape(batch, -1)\n",
    "            # compute next hidden: h_t = tanh(Wxh@x_t + Whh@h_{t-1} + bh)\n",
    "            pre = xt.dot(self.Wxh.T) + h[:, t, :].dot(self.Whh.T) + self.bh.T\n",
    "            h[:, t + 1, :] = np.tanh(pre)\n",
    "        # compute output using the last hidden state.\n",
    "        # NOTE:  We could also use multiple hidden states to have more context.\n",
    "        y_pred = h[:, -1, :].dot(self.Why.T) + self.by.T\n",
    "        return h, y_pred  # h includes initial zero state at index 0\n",
    "\n",
    "    def loss(self, y_pred, y_true):\n",
    "        # MSE\n",
    "        diff = y_pred - y_true\n",
    "        return np.mean(diff**2), diff\n",
    "\n",
    "    def bptt_update(self, X, h, y_pred, y_true):\n",
    "        # Using backpropagation through time (bptt) to update the weights\n",
    "        # X: (batch, seq_len, input_size)\n",
    "        batch, seq_len, _ = X.shape\n",
    "\n",
    "        # Initializing the gradients\n",
    "        dWxh = np.zeros_like(self.Wxh)\n",
    "        dWhh = np.zeros_like(self.Whh)\n",
    "        dWhy = np.zeros_like(self.Why)\n",
    "        dbh = np.zeros_like(self.bh)\n",
    "        dby = np.zeros_like(self.by)\n",
    "\n",
    "        # dy on outputs (MSE derivative)\n",
    "        dy = (y_pred - y_true) * (2.0 / batch)  # shape (batch, output_size)\n",
    "        # dWhy and dby from last hidden\n",
    "        # (batch, hidden)\n",
    "        h_last = h[:, -1, :].reshape(batch, self.hidden_size)\n",
    "        dWhy += dy.T.dot(h_last)  # (output, hidden)\n",
    "        dby += dy.T.sum(axis=1, keepdims=True)  # (output,1)\n",
    "\n",
    "        # backprop into last hidden state\n",
    "        dh_next = dy.dot(self.Why)  # (batch, hidden)\n",
    "\n",
    "        # BPTT through time\n",
    "        # NOTE: As in normal BP, we go the network backwards\n",
    "        for t in reversed(range(seq_len)):\n",
    "            ht = h[:, t + 1, :]  # (batch, hidden)\n",
    "            ht_prev = h[:, t, :]  # (batch, hidden)\n",
    "            # derivative through tanh\n",
    "            dt = dh_next * (1 - ht**2)  # (batch, hidden)\n",
    "            dbh += dt.T.sum(axis=1, keepdims=True)\n",
    "            # dWxh: sum over batch of dt^T x_t\n",
    "            xt = X[:, t, :].reshape(batch, -1)\n",
    "            dWxh += dt.T.dot(xt)  # (hidden, input)\n",
    "            # dWhh: dt^T h_{t-1}\n",
    "            dWhh += dt.T.dot(ht_prev)\n",
    "            # propagate dh to previous time step\n",
    "            dh_next = dt.dot(self.Whh)\n",
    "\n",
    "        # Gradient clipping to avoid exploding gradients.\n",
    "        for grad in (dWxh, dWhh, dWhy, dbh, dby):\n",
    "            np.clip(grad, -5, 5, out=grad)\n",
    "\n",
    "        # SGD parameter update\n",
    "        self.Wxh -= self.lr * dWxh\n",
    "        self.Whh -= self.lr * dWhh\n",
    "        self.Why -= self.lr * dWhy\n",
    "        self.bh -= self.lr * dbh\n",
    "        self.by -= self.lr * dby\n",
    "\n",
    "    def train(self, X, y, epochs=50, batch_size=32, verbose=True):\n",
    "        n = X.shape[0]\n",
    "        losses = []\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            # shuffle\n",
    "            idx = np.random.permutation(n)\n",
    "            X_shuffled = X[idx]\n",
    "            y_shuffled = y[idx]\n",
    "            epoch_loss = 0.0\n",
    "\n",
    "            # Creating batches, feeding through the network,\n",
    "            # computing loss, and updating the gradients\n",
    "            for i in range(0, n, batch_size):\n",
    "                xb = X_shuffled[i: i + batch_size]\n",
    "                yb = y_shuffled[i: i + batch_size]\n",
    "                h, y_pred = self.forward(xb)\n",
    "                loss, _ = self.loss(y_pred, yb)\n",
    "                epoch_loss += loss * xb.shape[0]\n",
    "                self.bptt_update(xb, h, y_pred, yb)\n",
    "            epoch_loss /= n\n",
    "            losses.append(epoch_loss)\n",
    "            if verbose and (epoch % max(1, epochs // 10) == 0 or epoch == 1):\n",
    "                print(f\"Epoch {epoch}/{epochs} - loss: {epoch_loss:.6f}\")\n",
    "        return losses\n",
    "\n",
    "def generate_sine_sequences(n_samples=2000, seq_len=20, input_size=1, seed=0):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    x = np.linspace(0, 50, n_samples * seq_len * input_size)\n",
    "    data = np.sin(x) + 0.1 * rng.randn(n_samples * seq_len * input_size)\n",
    "    X = data.reshape(n_samples, seq_len, input_size)\n",
    "    rolled = np.roll(data, -1).reshape(n_samples, seq_len, input_size)\n",
    "    y_last = rolled[:, -1, :]\n",
    "    return X.astype(np.float32), y_last.astype(np.float32)\n",
    "\n",
    "if False: \n",
    "    # Creating the data\n",
    "    X, y = generate_sine_sequences(n_samples=1500, seq_len=20, input_size=1)\n",
    "    # train/test split\n",
    "    split = int(0.8 * X.shape[0])\n",
    "    X_train, y_train = X[:split], y[:split]\n",
    "    X_test, y_test = X[split:], y[split:]\n",
    "\n",
    "    print(X_train.shape, y_train.shape)\n",
    "\n",
    "    # Training the network\n",
    "    rnn = RNN(input_size=1, hidden_size=80, output_size=1, lr=1e-3)\n",
    "    losses = rnn.train(X_train, y_train, epochs=600, batch_size=8, verbose=True)\n",
    "\n",
    "    # Test set\n",
    "    _, y_pred_test = rnn.forward(X_test)\n",
    "    test_loss, _ = rnn.loss(y_pred_test, y_test)\n",
    "    print(f\"\\nTest MSE: {test_loss:.6f}\\n\")\n",
    "\n",
    "    # Plotting the predictions.\n",
    "    plt.plot(y_test)\n",
    "    plt.plot(y_pred_test)\n",
    "    plt.savefig(\"rnn_pred.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45e2808",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## Classification model implementation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48876ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ClassificationRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, output_size):\n",
    "        super(ClassificationRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.RNN(embed_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        h0 = torch.zeros(1, x.size(0), hidden_size).to(x.device)\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92df6eab",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## Data onboarding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caff083a",
   "metadata": {},
   "source": [
    "Fetch the data from kaggle API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c14397a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2225, 2)\n",
      "                                                Text  Label\n",
      "0  Budget to set scene for election\\n \\n Gordon B...      0\n",
      "1  Army chiefs in regiments decision\\n \\n Militar...      0\n",
      "2  Howard denies split over ID cards\\n \\n Michael...      0\n",
      "3  Observers to monitor UK election\\n \\n Minister...      0\n",
      "4  Kilroy names election seat target\\n \\n Ex-chat...      0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import kagglehub\n",
    "\n",
    "# Fetch the latest version of the dataset from kaggle\n",
    "data_dir = kagglehub.dataset_download(\"tanishqdublish/text-classification-documentation\")\n",
    "data_path = os.path.join(data_dir, os.listdir(data_dir)[0]);\n",
    "\n",
    "data = pd.read_csv(data_path)\n",
    "print(data.shape)\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae6f926",
   "metadata": {},
   "source": [
    "Split the data to features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc3305cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data['Text'], data['Label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15fc008",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b0a05d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/eliaseskelinen/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "\n",
    "    # Lowercase and remove non-alphabetic characters\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    words = text.split()\n",
    "\n",
    "    # Lemmatization\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "    # Stop-word removal\n",
    "    # words = [word for word in words if word not in stopwords]\n",
    "\n",
    "    return text #' '.join(words)\n",
    "\n",
    "# Apply the cleaning function\n",
    "X = X.apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2de2d5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# import re\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# # # Tokenize each sentence\n",
    "# # for i, x in enumerate(X): \n",
    "# #     #tokens = nltk.word_tokenize(x)\n",
    "# #     tokens = nltk.word_tokenize(x)\n",
    "# #     X[i] = ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9d7b378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2225,)\n"
     ]
    }
   ],
   "source": [
    "X, y = X.to_numpy(), y.to_numpy()\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b63f57",
   "metadata": {},
   "source": [
    "Vectorize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0c92b505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Basic tokenization\n",
    "def tokenize(sentence):\n",
    "    return re.findall(r\"\\b\\w+\\b\", sentence.lower())\n",
    "\n",
    "tokenized = [tokenize(s) for s in X]\n",
    "\n",
    "# Build vocabulary\n",
    "vocab = {}\n",
    "for sent in tokenized:\n",
    "    for word in sent:\n",
    "        if word not in vocab:\n",
    "            vocab[word] = len(vocab) + 1  # start indexing at 1\n",
    "\n",
    "sequences = [[vocab[word] for word in sent] for sent in tokenized]\n",
    "# e.g., [[1, 2, 3, 4, 1, 5], [6, 7, 8, 9], [10, 11, 12]]\n",
    "\n",
    "max_len = max(len(seq) for seq in sequences)\n",
    "padded = np.zeros((len(sequences), max_len), dtype=int)\n",
    "\n",
    "for i, seq in enumerate(sequences):\n",
    "    padded[i, :len(seq)] = seq\n",
    "# shape = (batch, seq_len)\n",
    "\n",
    "vocab_size = len(vocab) + 1  # +1 for padding index 0\n",
    "embedding_dim = 300  # features per token\n",
    "\n",
    "embedding_matrix = np.random.randn(vocab_size, embedding_dim)\n",
    "embeddings = embedding_matrix[padded]  # shape = (batch, seq_len, features)\n",
    "\n",
    "rnn_input = np.transpose(embeddings, (2, 1, 0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047870d0",
   "metadata": {},
   "source": [
    "Split the data to training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c1731b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(rnn_input, y, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a64ad6f",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac8e786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 4396, 2225) (2225,)\n"
     ]
    }
   ],
   "source": [
    "rnn = RNN(input_size=1, hidden_size=80, output_size=1, lr=1e-3)\n",
    "\n",
    "print(rnn_input.shape, y.shape)\n",
    "rnn.train(rnn_input, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e73985",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/content/IMDB Dataset.csv\", names=[\"text\", \"label\"])\n",
    "\n",
    "le = LabelEncoder()\n",
    "df['label'] = le.fit_transform(df['label'])\n",
    "\n",
    "train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "vocab = {word for phrase in df['text'] for word in phrase}\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocab, start=1)}\n",
    "\n",
    "max_length = df['text'].str.len().max()\n",
    "\n",
    "def encode_and_pad(text):\n",
    "    encoded = [word_to_idx[word] for word in text]\n",
    "    return encoded + [0] * (max_length - len(encoded))\n",
    "\n",
    "train_data['text'] = train_data['text'].apply(encode_and_pad)\n",
    "test_data['text'] = test_data['text'].apply(encode_and_pad)\n",
    "vocab_size = len(vocab) + 1\n",
    "embed_size = 128\n",
    "hidden_size = 128\n",
    "output_size = 2 \n",
    "model = ClassificationRNN(vocab_size, embed_size, hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22aa8c3",
   "metadata": {},
   "source": [
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
